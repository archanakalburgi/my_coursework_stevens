{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Word_Vector.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"SW1urolMc2Yx","executionInfo":{"status":"ok","timestamp":1638235687821,"user_tz":300,"elapsed":266,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}}},"source":["# softmax -> one hot coding -> probability distribution\n","# assigns small probability values to the non max class"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2jlglJSFW2iG"},"source":["## <center>Word Vector (a.k.a Word Embedding) </center> "]},{"cell_type":"markdown","metadata":{"id":"v_EhZJsVW2iM"},"source":["### 1.1 Word2Vector\n"," - Vector representation of words (i.e. word vectors) learned using neural network\n","   - e.g. \"apple\" : [0.35, -0.2, 0.4, ...], 'mongo':  [0.32, -0.18, 0.5, ...]\n","   - Interesting properties of word vectors:\n","    * **Words with similar semantics have close word vectors**\n","    <img src=\"https://www.kdnuggets.com/images/cartoon-espresso-word2vec.jpg\" width=\"50%\">\n","    https://www.kdnuggets.com/2017/04/cartoon-word2vec-espresso-cappuccino.html\n","    * **Composition**: e.g. vector(\"woman\")+vector(\"king\")-vector('man') $\\approx$ vector(\"queen\")\n"," - Models:\n","   - **CBOW** (Continuous Bag of Words): Predict a target word based on context\n","     - e.g. the fox jumped over the lazy dog\n","     - Assuming symmetric context with window size 3, this sentence can create training samples: \n","       - ([-, fox], the) \n","       - ([the, jumped], fox) \n","       - ([fox, over], jumped)\n","       - ([jumped, the], over) \n","       - ...\n","       \n","       <img src=\"cbow.png\" width=\"50%\">\n","       source: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n","   - **Skip Gram**: predict context based on target words\n","   \n","        <img src=\"skip_gram.png\" width=\"50%\">\n","        source: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6OlTaO4dW2iO","executionInfo":{"status":"ok","timestamp":1638236325545,"user_tz":300,"elapsed":854,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"af9b6800-f460-4a26-b5dc-8e538c101f7d"},"source":["# set up interactive shell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","import nltk\n","nltk.download('punkt')"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":243},"id":"oyEVP-w1W2iR","executionInfo":{"status":"ok","timestamp":1638236343776,"user_tz":300,"elapsed":14447,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"1278a0c5-725f-473e-a50a-c29c93171a29"},"source":["# Exercise 1.1 Train your word vector\n","\n","import pandas as pd\n","import nltk,string\n","\n","# Load data\n","data=pd.read_csv('/content/drive/MyDrive/BIA_660_Web_Mining/in_class/word_vector/amazon_review_large.csv')\n","data.columns=['label','text']\n","data.head()\n","\n","# tokenize each document into a list of unigrams\n","# strip punctuations and leading/trailing spaces from unigrams\n","# only unigrams with 2 or more characters are taken\n","sentences=[ [token.strip(string.punctuation).strip() \\\n","             for token in nltk.word_tokenize(doc.lower()) \\\n","                 if token not in string.punctuation and \\\n","                 len(token.strip(string.punctuation).strip())>=2]\\\n","             for doc in data[\"text\"]]\n","print(sentences[0:2])"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>This is a little longer and more detailed than...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Only Michelle Branch save this album!!!!All gu...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>A surprisingly good book, given its inherently...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>This is a wonderful, quiet and relaxing CD tha...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>The lights that I received are absolutely not ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                               text\n","0      2  This is a little longer and more detailed than...\n","1      1  Only Michelle Branch save this album!!!!All gu...\n","2      2  A surprisingly good book, given its inherently...\n","3      2  This is a wonderful, quiet and relaxing CD tha...\n","4      1  The lights that I received are absolutely not ..."]},"metadata":{},"execution_count":7},{"output_type":"stream","name":"stdout","text":["[['this', 'is', 'little', 'longer', 'and', 'more', 'detailed', 'than', 'the', 'first', 'two', 'books', 'in', 'the', 'series', 'however', 'have', 'enjoyed', 'each', 'new', 'aspect', 'of', 'the', 'exciting', 'fantasy', 'universe'], ['only', 'michelle', 'branch', 'save', 'this', 'album', 'all', 'guys', 'play', 'along', 'with', 'unenthusiastic', 'beat', 'even', 'karl']]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-X3sXr0fT25","executionInfo":{"status":"ok","timestamp":1638236270107,"user_tz":300,"elapsed":25969,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"4428f7af-e8ac-4a7f-9812-194356b4ba1a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":200},"id":"EAzkYcH3W2iS","executionInfo":{"status":"error","timestamp":1638236446379,"user_tz":300,"elapsed":322,"user":{"displayName":"Archana Kalburgi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07335954375421111860"}},"outputId":"24ba4606-b920-4227-9f41-d1b32737153d"},"source":["# Train your own word vectors using gensim\n","\n","# gensim.models is the package for word2vec\n","# check https://radimrehurek.com/gensim/models/word2vec.html\n","# for detailed description\n","\n","from gensim.models import word2vec\n","import logging\n","import pandas as pd\n","\n","# print out tracking information\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n","                    level=logging.INFO)\n","\n","# min_count: words with total frequency lower than this are ignored\n","# size: the dimension of word vector\n","# window: context window, i.e. the maximum distance \n","#         between the current and predicted word \n","#         within a sentence (i.e. the length of ngrams)\n","# workers: # of parallel threads in training\n","# for other parameters, check https://radimrehurek.com/gensim/models/word2vec.html\n","wv_model = word2vec.Word2Vec(sentences, min_count=5, vector_size=200, window=5, workers=4 )"],"execution_count":9,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-07247f72f8a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# workers: # of parallel threads in training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# for other parameters, check https://radimrehurek.com/gensim/models/word2vec.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mwv_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'vector_size'"]}]},{"cell_type":"code","metadata":{"id":"rgvFP5z5W2iT","outputId":"37959922-b9e3-447e-f977-2bfd29bc8b70"},"source":["# test word2vec model\n","\n","print(\"Top 5 words similar to word 'sound'\")\n","wv_model.wv.most_similar('sound', topn=5)\n","# wv is a dictinary \n","\n","print(\"Top 5 words similar to word 'sound' but not relevant to 'film'\")\n","wv_model.wv.most_similar(positive=['sound','music'], \\\n","                         negative=['film'], topn=5)\n","\n","print(\"Similarity between 'movie' and 'film':\")\n","wv_model.wv.similarity('movie','film') \n","\n","print(\"Similarity between 'movie' and 'city':\")\n","wv_model.wv.similarity('movie','city') \n","\n","print(\"Word does not match with others in the list of \\\n","['sound', 'music', 'graphics', 'actor', 'book']:\")\n","wv_model.wv.doesnt_match([\"sound\", \"music\", \\\n","                          \"graphics\", \"actor\", \"book\"])\n","\n","print(\"Word vector for 'movie':\")\n","wv_model.wv['movie'] #-> len=200, len of dimensions"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Top 5 words similar to word 'sound'\n"]},{"data":{"text/plain":["[('band', 0.7466944456100464),\n"," ('metal', 0.7309845089912415),\n"," ('production', 0.7275912165641785),\n"," ('vocals', 0.7178143858909607),\n"," ('beats', 0.7157869338989258)]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["Top 5 words similar to word 'sound' but not relevant to 'film'\n"]},{"data":{"text/plain":["[('pop', 0.7688503265380859),\n"," ('rock', 0.7571867108345032),\n"," ('guitar', 0.7289978265762329),\n"," ('songs', 0.7192723751068115),\n"," ('invisible', 0.7177398800849915)]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["Similarity between 'movie' and 'film':\n"]},{"data":{"text/plain":["0.92379814"]},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["Similarity between 'movie' and 'city':\n"]},{"data":{"text/plain":["0.036855552"]},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["Word does not match with others in the list of ['sound', 'music', 'graphics', 'actor', 'book']:\n"]},{"data":{"text/plain":["'book'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["Word vector for 'movie':\n"]},{"data":{"text/plain":["array([-1.802832  ,  0.24956171, -0.11756705,  0.04693254, -1.5093597 ,\n","        0.5086073 ,  0.3910569 , -0.89214   ,  0.23388013, -1.2573065 ,\n","        0.6647229 ,  1.7413628 ,  0.81334066, -0.9406785 , -1.178873  ,\n","        0.68965447,  1.0267875 , -1.4296671 , -1.1222117 , -1.606071  ,\n","        1.7382059 ,  0.56719065,  0.04122837,  0.22017027, -0.3461662 ,\n","       -0.13749823, -0.9189834 , -1.0497375 ,  0.7365495 ,  0.09684153,\n","        0.30657044,  0.7274297 ,  0.5823868 ,  0.03434692, -1.6505245 ,\n","        2.875775  , -1.5738666 , -0.4178427 , -1.1819825 , -0.4639121 ,\n","        0.55049515, -1.1630489 ,  0.54714614, -0.92406774,  0.7725745 ,\n","        0.2697898 , -0.9988751 , -0.14581019, -0.04652194,  0.31032822,\n","        0.29798335, -0.36748606,  0.33740562, -0.04714237, -1.3902302 ,\n","       -1.067852  , -0.7876221 , -2.2594395 , -1.5850363 ,  1.1617929 ,\n","        1.4588189 , -0.04304981,  0.71149564, -0.55700463, -2.5268452 ,\n","       -1.3418169 ,  0.26387188,  0.10013806, -0.73470455,  0.39770612,\n","        0.5065341 ,  1.4594344 ,  0.95209855,  0.44258544,  0.19725145,\n","       -1.0049571 ,  2.3528962 ,  0.01366105, -1.6356779 ,  0.173993  ,\n","       -0.07093949, -0.82312334, -1.2965604 , -0.14975081, -0.53450626,\n","       -0.03705219,  1.5994643 ,  0.6313663 , -1.0390366 ,  0.30132937,\n","        0.23536944,  0.8956761 ,  0.2444061 , -0.11600939, -0.52704763,\n","        0.67840564, -0.8954986 , -0.82062966,  0.4964672 ,  1.4842579 ,\n","        0.42322704, -0.4093063 ,  1.4624926 , -1.8591985 ,  1.4679621 ,\n","        0.48699275, -0.3030638 , -1.3160024 , -0.13226482,  1.3639237 ,\n","        0.520138  , -1.5730096 ,  0.6063167 , -0.77484006,  0.75499636,\n","        0.47844356,  2.207294  , -0.25039652,  0.1397654 ,  0.55232614,\n","        0.089279  ,  1.3653619 ,  1.2556955 ,  1.4863821 , -1.0511409 ,\n","        0.64893514,  0.8396899 ,  0.9816944 ,  0.3234426 , -0.1524925 ,\n","        1.8199936 , -0.5520708 ,  0.3710983 , -1.0201043 ,  0.62848455,\n","       -1.2944022 ,  0.6626879 ,  0.8038365 , -0.42785102, -0.16223024,\n","       -0.11335327,  0.27161136, -0.12791432, -1.5311518 , -1.8293543 ,\n","       -0.39491704, -0.5337843 ,  0.7758201 ,  0.57505876, -1.4777431 ,\n","       -0.04070774, -1.086612  ,  1.6036246 , -0.9545042 ,  0.7474546 ,\n","        0.30606785,  0.35414743,  2.3398755 , -0.55985886, -0.18651076,\n","        0.4732579 , -0.29538503, -1.6896963 ,  2.8892956 ,  0.4342756 ,\n","        1.3679063 , -0.61184824,  1.0790496 ,  0.39981142, -0.9822273 ,\n","       -1.0549433 ,  1.2320508 , -0.06196262, -1.6105052 , -1.425723  ,\n","        0.32382235,  0.00531442,  1.7085718 ,  0.79473275, -0.21838301,\n","       -1.1002159 , -1.4858607 , -0.05873797,  0.3012048 ,  0.69434536,\n","        0.41379994, -0.8324041 , -0.5401535 ,  0.5301314 ,  0.55243605,\n","       -1.2018309 , -0.84638935, -0.20272726, -1.0722165 ,  1.5638922 ,\n","        0.95433086, -0.37784982, -0.03345486, -1.0710598 ,  0.26041618],\n","      dtype=float32)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"1jYlpo1TW2iV"},"source":["### 1.2. Pretrained Word Vectors\n","- Google published pre-trained 300-dimensional vectors for 3 million words and phrases that were trained on Google News dataset (about 100 billion words)(https://code.google.com/archive/p/word2vec/)\n","- GloVe (Global Vectors for Word Representation): Pretained word vectors from different data sources provided by Standford https://nlp.stanford.edu/projects/glove/\n","- FastText by Facebook https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md"]},{"cell_type":"code","metadata":{"id":"N1w_5k5ZW2iW","outputId":"be9e2004-22e2-4066-a02c-1363f8a8adae"},"source":["# Exercise 1.2: Use pretrained word vectors\n","\n","# download the bin file for pretrained word vectors\n","# from above links, e.g. https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n","# Warning: the bin file is very big (over 2G)\n","# You need a powerful machine to load it\n","\n","import gensim\n","\n","model = gensim.models.KeyedVectors.\\\n","load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) \n","\n","\n"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["2021-11-26 21:16:27,886 : INFO : loading projection weights from GoogleNews-vectors-negative300.bin\n","2021-11-26 21:17:01,517 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from GoogleNews-vectors-negative300.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2021-11-26T21:17:01.517006', 'gensim': '4.1.2', 'python': '3.7.6 (default, Jan  8 2020, 13:42:34) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'load_word2vec_format'}\n"]}]},{"cell_type":"code","metadata":{"id":"hzoAVcBpW2iX","outputId":"cac93ba5-528b-4df9-88d7-a603481ed9de"},"source":["model.most_similar(positive=['women','king'], \\\n","                      negative='man')"],"execution_count":null,"outputs":[{"data":{"text/plain":["[('queen', 0.4827326238155365),\n"," ('queens', 0.466781347990036),\n"," ('kumaris', 0.4653734564781189),\n"," ('kings', 0.4558638632297516),\n"," ('womens', 0.422832190990448),\n"," ('princes', 0.4176960587501526),\n"," ('Al_Anqari', 0.41725507378578186),\n"," ('concubines', 0.4011078476905823),\n"," ('monarch', 0.3962482810020447),\n"," ('monarchy', 0.39430150389671326)]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"wonQhPs5W2iY"},"source":["### 1.3. How to use word vectors in classification?\n","\n","`Convolutional Neural Network`\n","<img src=\"CNN.png\" width =\"100%\">\n","\n","`Recurrent Neural Network`\n","\n","<img src=\"https://raw.githubusercontent.com/graviraja/100-Days-of-NLP/master/assets/images/applications/sentiment/simple.gif\" width = \"90%\">"]},{"cell_type":"markdown","metadata":{"id":"EuFqL-7gW2ia"},"source":["<img src=\"https://www.kdnuggets.com/images/cartoon-machine-learning-vacation.jpg\" width='60%'>\n"]}]}