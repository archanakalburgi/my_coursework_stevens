{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FDA_pima_indians.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1v83geSj2oT9Fazngn2bxi81Kd0-I4adB","authorship_tag":"ABX9TyMwwPcPbzn39rXuHlzblC3C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6CGEMXrhyX1u"},"source":["**Name : Archana Kalburgi**\n","\n","**CWID : 10469491**\n","\n","**Solution to Question 3**"]},{"cell_type":"markdown","metadata":{"id":"Y4U-EPXoxaRR"},"source":["Steps involved:\n","\n","1. Load and read the pima-indians-diabetes data \n","\n","2. Calculate the mean vectors for different classes \n","\n","3. Calculate Within-class scatter matrix s_w\n","\n","4. Calculate Between-class scatter matrix s_b\n","\n","5. Solve the generalized eigenvalue problem for the matrix\n","\n","6. Select linear discriminants for the new feature subspace\n","\n","7. Sort the eigenvectors by decreasing eigenvalues\n","\n","8. Choose k eigenvectors with the largest eigenvalues\n","\n","9. Transform the samples onto the new subspace\n","\n","10. Train a classifier using MLE after the data have been projected.\n","\n","11. Report accuracy for 10 runs by randomly splitting the data "]},{"cell_type":"code","metadata":{"id":"qJuRZz2c52a4"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ec2-NIa5_qu"},"source":["# data\n","data_raw = pd.read_csv(\"/content/drive/MyDrive/ML_Assignments/ML_Assign_test/pima-indians-diabetes.csv\")\n","\n","c0_label = data_raw[data_raw[\"Class variable\"]==0]\n","c0 = c0_label.iloc[:, :-1].to_numpy()\n","\n","c1_label = data_raw[data_raw[\"Class variable\"]==1]\n","c1 = c1_label.iloc[:, :-1].to_numpy() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hPb-zCHIrMGk"},"source":["# mean vectors for different classes\n","mean0 = c0.mean(axis=0)\n","mean1 = c1.mean(axis=0)\n","data = data_raw.iloc[:, :-1].to_numpy()\n","mean_overall = data.mean(axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"chY2A3WOkk53"},"source":["# Calculate Within-class scatter matrix s_w\n","features = data.shape[1]\n","s_w0 = (c0 - mean0).T.dot(c0 - mean0)\n","s_w1 = (c1 - mean1).T.dot(c1 - mean1)\n","\n","s_w = s_w0 + s_w1\n","s_w"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kjr2Dmw0k8Yp","executionInfo":{"status":"ok","timestamp":1620757245857,"user_tz":240,"elapsed":418,"user":{"displayName":"Archana Kalburgi","photoUrl":"","userId":"07335954375421111860"}},"outputId":"bdfc64c4-7757-4976-f400-0ea20bc37641"},"source":["# Calculate Between-class scatter matrix s_b\n","mean_diff0 = (mean0 - mean_overall).reshape(features, 1)\n","mean_diff1 = (mean1 - mean_overall).reshape(features, 1)\n","\n","nc0 = c0.shape[0]\n","nc1 = c1.shape[0]\n","\n","s_b0 = nc0 * mean_diff0.dot(mean_diff0.T)\n","s_b1 = nc1 * mean_diff1.dot(mean_diff1.T)\n","\n","s_b = s_b0 + s_b1\n","s_b"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[4.28799019e+02, 8.55520056e+03, 7.22280214e+02, 6.83864094e+02,\n","        8.62805647e+03, 1.32341125e+03, 3.30326459e+01, 1.60755745e+03],\n","       [8.55520056e+03, 1.70689422e+05, 1.44106022e+04, 1.36441415e+04,\n","        1.72143009e+05, 2.64040919e+04, 6.59052140e+02, 3.20732460e+04],\n","       [7.22280214e+02, 1.44106022e+04, 1.21662757e+03, 1.15191846e+03,\n","        1.45333226e+04, 2.22918832e+03, 5.56410475e+01, 2.70781156e+03],\n","       [6.83864094e+02, 1.36441415e+04, 1.15191846e+03, 1.09065105e+03,\n","        1.37603347e+04, 2.11062385e+03, 5.26816515e+01, 2.56379043e+03],\n","       [8.62805647e+03, 1.72143009e+05, 1.45333226e+04, 1.37603347e+04,\n","        1.73608976e+05, 2.66289486e+04, 6.64664615e+02, 3.23463811e+04],\n","       [1.32341125e+03, 2.64040919e+04, 2.22918832e+03, 2.11062385e+03,\n","        2.66289486e+04, 4.08447144e+03, 1.01949336e+02, 4.96143772e+03],\n","       [3.30326459e+01, 6.59052140e+02, 5.56410475e+01, 5.26816515e+01,\n","        6.64664615e+02, 1.01949336e+02, 2.54467863e+00, 1.23838614e+02],\n","       [1.60755745e+03, 3.20732460e+04, 2.70781156e+03, 2.56379043e+03,\n","        3.23463811e+04, 4.96143772e+03, 1.23838614e+02, 6.02669515e+03]])"]},"metadata":{"tags":[]},"execution_count":106}]},{"cell_type":"code","metadata":{"id":"1QwmOJ3qlbR4"},"source":["# Solve the generalized eigenvalue problem for the matrix\n","A = np.linalg.inv(s_w).dot(s_b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZnnED6tIl8UT"},"source":["# Select linear discriminants for the new feature subspace\n","evalues, evector = np.linalg.eig(A)\n","evector = evector.T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cjp355WNl_9j","executionInfo":{"status":"ok","timestamp":1620757246017,"user_tz":240,"elapsed":559,"user":{"displayName":"Archana Kalburgi","photoUrl":"","userId":"07335954375421111860"}},"outputId":"6b8d8c61-5eb5-45a7-c7ad-01028629a8f0"},"source":["# Sort the eigenvectors by decreasing eigenvalues\n","idx = np.argsort(abs(evalues))[::-1]\n","evalues = evalues[idx]\n","evector = evector[idx]\n","\n","# Choose k eigenvectors with the largest eigenvalues\n","discriminants = evector[0:1] # bcoz we have on 2 dimensions, optimal direction\n","\n","# Transform the samples onto the new subspace -> project the data\n","x_projected = np.dot(data, discriminants.T)\n","len(x_projected)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["768"]},"metadata":{"tags":[]},"execution_count":109}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"id":"5TgXam8Om5ic","executionInfo":{"status":"ok","timestamp":1620757246019,"user_tz":240,"elapsed":549,"user":{"displayName":"Archana Kalburgi","photoUrl":"","userId":"07335954375421111860"}},"outputId":"28ab17a3-a642-4b5d-bc6b-d8bb090090ca"},"source":["target = data_raw.iloc[:, 8]\n","\n","ddf = pd.DataFrame(x_projected, columns = [\"FDA\"])\n","ddf \n","\n","df_with_class = pd.concat([ddf, pd.DataFrame(target)], axis= 1)\n","df_with_class\n","\n","# data_raw.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>FDA</th>\n","      <th>Class variable</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-10.076782+0.000000j</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-5.753172+0.000000j</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-10.643429+0.000000j</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-5.568082+0.000000j</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-11.291041+0.000000j</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>763</th>\n","      <td>-8.216005+0.000000j</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>764</th>\n","      <td>-8.115602+0.000000j</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>765</th>\n","      <td>-7.338209+0.000000j</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>766</th>\n","      <td>-8.030298+0.000000j</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>767</th>\n","      <td>-6.170788+0.000000j</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>768 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                     FDA  Class variable\n","0   -10.076782+0.000000j               1\n","1    -5.753172+0.000000j               0\n","2   -10.643429+0.000000j               1\n","3    -5.568082+0.000000j               0\n","4   -11.291041+0.000000j               1\n","..                   ...             ...\n","763  -8.216005+0.000000j               0\n","764  -8.115602+0.000000j               0\n","765  -7.338209+0.000000j               0\n","766  -8.030298+0.000000j               1\n","767  -6.170788+0.000000j               0\n","\n","[768 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":110}]},{"cell_type":"code","metadata":{"id":"o3P6VhNRoOvk"},"source":["# Train a classifier using MLE after the data have been projected.\n","\n","def noramal_eq(xi, mu, sigma_inv, scalar):\n","  pp = (-1/2)*np.dot(np.matmul(xi - mu, sigma_inv), xi - mu)\n","  return scalar * (np.e**pp)\n","\n","# calculating mean, sigma, sigma_inverse, scalar\n","def components(x):\n","  mu = np.mean(x, axis=0)\n","  sigma = np.cov(x, rowvar=False)\n","  sigma_inv = np.linalg.inv(sigma)\n","  scalar = 1/np.sqrt(((2*np.pi)**x.shape[1])*np.linalg.det(sigma))\n","  return (mu, sigma_inv, scalar)\n","\n","# computing likelihood\n","def likelihood(x, mu, sigma_inv, scalar):\n","  return [noramal_eq(x, mu, sigma_inv, scalar) for x in range(x.shape[0])]\n","\n","# computing the accuracy \n","def predit(train_x, train_y, test_x, test_y):\n","  # compute train accuracy(on train data)\n","  mu0, sigma_inv0, scalar0 = components(train_x)\n","  l0 = likelihood(train_x, mu0, sigma_inv0, scalar0 )\n","  mu1, sigma_inv1, scalar1 = components(train_x)\n","  l1 = likelihood(train_x, mu1, sigma_inv1, scalar1)\n","\n","  predicted_y = np.array([1 if ll1 > ll0 else 0 for (ll0, ll1) in zip(l0, l1)])\n","  train_n_correct = sum([1 if predicted_y[i] == train_y[i] else 0 for i in range(train_y.shape[0])])\n","  \n","  # compute test accuracy (on test data)\n","  tl_0 = likelihood(test_x, mu0, sigma_inv0, scalar0 )\n","  tl_1 = likelihood(test_x, mu1, sigma_inv1, scalar1 )\n","\n","  predited_test_y = np.array([1 if ll1 > ll0 else 0 for (ll0, ll1) in zip(tl_0, tl_1)])\n","  test_n_correct = sum([1 if predicted_y[i] == test_y[i] else 0 for i in range(test_y.shape[0])])\n","\n","  return (train_n_correct / train_y.shape[0], test_n_correct / test_y.shape[0]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0O_8APXoXUA","executionInfo":{"status":"ok","timestamp":1620757337585,"user_tz":240,"elapsed":513,"user":{"displayName":"Archana Kalburgi","photoUrl":"","userId":"07335954375421111860"}},"outputId":"399c7a2d-64fe-4687-f1ba-14890d5d03a2"},"source":["from sklearn.model_selection import train_test_split\n","\n","train_10scores = []\n","test_10scores = []\n","for i in range(1,11):\n","    print(\"----------------------------------------\")\n","    (x_train, x_test, y_train, y_test) = train_test_split(df_with_class, df_with_class.iloc[:,1], train_size=0.5)\n","    train_score, test_score = predit(x_train.iloc[:,0:3].to_numpy(), y_train.to_numpy(), x_test.iloc[:,0:3].to_numpy(), y_test.to_numpy())\n","    np.array(train_10scores.append(train_score))\n","    np.array(test_10scores.append(test_score))\n","    print(f\"Train Score is {train_score}\")\n","    print(f\"Test Score is {test_score}\") \n","\n","# printing out the mean and standard deviation of all the 10 accuracy scores of train and test data \n","print(\"-------------------------------------------------\")\n","print(\"\\n\")\n","# print(f\"Mean of the accuracies for train data = {np.mean(train_10scores)}\")\n","print(f\"Average classification accuracy over 10 runs = {round(np.mean(test_10scores),4)}\") \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------\n","Train Score is 0.6640625\n","Test Score is 0.6380208333333334\n","----------------------------------------\n","Train Score is 0.6588541666666666\n","Test Score is 0.6432291666666666\n","----------------------------------------\n","Train Score is 0.65625\n","Test Score is 0.6458333333333334\n","----------------------------------------\n","Train Score is 0.6302083333333334\n","Test Score is 0.671875\n","----------------------------------------\n","Train Score is 0.6510416666666666\n","Test Score is 0.6510416666666666\n","----------------------------------------\n","Train Score is 0.6354166666666666\n","Test Score is 0.6666666666666666\n","----------------------------------------\n","Train Score is 0.6588541666666666\n","Test Score is 0.6432291666666666\n","----------------------------------------\n","Train Score is 0.6484375\n","Test Score is 0.6536458333333334\n","----------------------------------------\n","Train Score is 0.6666666666666666\n","Test Score is 0.6354166666666666\n","----------------------------------------\n","Train Score is 0.6536458333333334\n","Test Score is 0.6484375\n","-------------------------------------------------\n","\n","\n","Average classification accuracy over 10 runs = 0.6497\n"],"name":"stdout"}]}]}